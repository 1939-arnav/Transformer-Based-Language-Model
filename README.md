# Transformer-Based-Language-Model
Built a Transformer-based language model using PyTorch, inspired by the ’Attention Is All You Need’ research paper, to predict and generate text sequences
◦ Implemented key components of the Transformer architecture, including multi-head self-attention, feedforward layers, and
positional embeddings
◦ Designed a decoder structure to process sequences and generate tokens iteratively
◦ Trained the model on Shakespearean text, achieving coherent text generation and demonstrating the effectiveness of attention
mechanisms
◦ Optimized training using efficient batching and loss evaluation techniques
